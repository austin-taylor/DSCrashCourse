{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import tree\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Decision Tree Worksheet\n",
    "This worksheet will walk you through the process of creating a decision tree classifer which you will use to determine whether a given URL is malicious or not.  Once you have implemented the classifier, the worksheet will walk you through evaluating your model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 1:  Get your data\n",
    "The first step in any data science project is gathering the data.  In the ```Data``` folder, you will find a file called ```urlData2.csv``` which contains a series of URLs known to be malicious and other known not to be malicious.  The code we used to actually build this dataset is included below:\n",
    "```python\n",
    "whitelist = pd.read_csv( '../Data/url-whitelist.csv', names=['rank','url'])\n",
    "whitelist = whitelist.drop( 'rank', axis=1)\n",
    "whitelist = whitelist.sample( 400 )\n",
    "whitelist['isMalicious'] = 0\n",
    "\n",
    "blacklist = pd.read_csv( '../Data/url-blacklist', names=['url','something'] )\n",
    "blacklist = blacklist.drop( 'something', axis=1)\n",
    "blacklist = blacklist.sample(400)\n",
    "blacklist['isMalicious'] = 1\n",
    "urlData = pd.concat( [whitelist, blacklist], ignore_index=True )\n",
    "\n",
    "def getUrlData( row ):\n",
    "    try:\n",
    "        data = pd.read_json( 'https://whois.apitruck.com/:' + row['url'] )\n",
    "        data = data.drop( 'error', axis=1 )\n",
    "        data = data.T\n",
    "        row['created'] = data['created'].iloc[0]\n",
    "        row['changed'] = data['changed'].iloc[0]\n",
    "        row['expires'] = data['expires'].iloc[0]\n",
    "    \n",
    "        row['dnssec'] = data['dnssec'].iloc[0]\n",
    "        return row\n",
    "    except:\n",
    "        print( \"Could not find: \" + row['url'] )\n",
    "        row['created'] = False\n",
    "        row['changed'] = False\n",
    "        row['expires'] = False\n",
    "        return row\n",
    "\n",
    "\n",
    "urlData = urlData.apply( getUrlData, axis=1, reduce=False )\n",
    "urlData.to_csv( 'urlData2.csv' )\n",
    "```\n",
    "\n",
    "Read in the data, remove any unneeded columns, and convert any dates to datetime columns.  This data is meant to simulate real world data, so there are invalid dates in the date column.  You can either drop the invalid dates, or replace the invalid dates with some marker.  I'd recommend reading the documentation for the ```.to_datetime()``` method (http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html), especially the ```coerce``` option.  Store your data in a DataFrame called ```urlData```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 2:  Add Features\n",
    "Now that you have your data in memory, you need to extract features from the URL that you will later use for the classification exercise.  To begin with, use the techniques you have learned to extract the following features:\n",
    "- Time difference between creation and expiration \n",
    "- The url suffix (ie: .com, .org)\n",
    "- The number of periods in the URL\n",
    "- The number of digits\n",
    "- The position (or index) of the first digit\n",
    "\n",
    "Feel free to add additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def firstDigitIndex( s ):\n",
    "    for i, c in enumerate(s):\n",
    "        if c.isdigit():\n",
    "            return i\n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 3: Vectorize your Data\n",
    "Once you have extracted our features from the dataset, you now need to vectorize the categorical columns, and remove any extraneous columns from the dataset.  In this instance, extraneous columns are any columns that are not features or the target column.  You can pretty much adapt code from the Play Golf example, but the basic steps are:\n",
    "1.  Identify the columns which contain categorical data\n",
    "2.  Extract those columns and convert them to a dictionary\n",
    "3.  Use scikit-learn's ```DictVectorizer``` to vectorize the data\n",
    "4.  Create a DataFrame with the Vectorized data\n",
    "5.  Merge the vectorized data with the original dataset\n",
    "6.  Drop the categorical columns and any other unneeded columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Identify Columns which contain categorical data\n",
    "\n",
    "\n",
    "#Convert those columns to a dictionary\n",
    "\n",
    "\n",
    "#Use the scikit-learn dict vectorizer to encode these columsn\n",
    "\n",
    "\n",
    "#Convert this back to a dataframe with the appropriate column names\n",
    "\n",
    "#Get the vectorized column names\n",
    "\n",
    "#Add those columns to the dataframe vector\n",
    "\n",
    "#duplicate the index\n",
    "\n",
    "#Drop unneeded columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 4:  Split the data\n",
    "Next, we need to divide the dataset into two dataframes, one containing the features, and the other containing the target column.  Traditionally, many machine learning texts refer to these as X and Y respectively, however, I believe that using unclear variable names is an excellent way to introduce errors to your code, as well as generating unreadable code.  Therefore I will name these as ```features``` and ```labels``` respectively. \n",
    "\n",
    "Finally, we will need to randomly divide the dataset into a training and testing set.  To do this, use the ```train_test_split()``` function within scikit-learn.  For our purposes, divide the data up by 75/25 training to test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data set into features and labels\n",
    "\n",
    "#Split the data set into training and test data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 5:  Train the Classifier\n",
    "Finally, we have prepared and segmented the data. Let's start classifying!!  \n",
    "1.  Use the scikit-learn ```DecisionTreeClassfier()```, create a decision tree, and train it\n",
    "2.  Next, pull a few random rows from the data and see if your classifier got it correct.\n",
    "If you are interested in trying a real unknown URL, you'll have to create a function to generate the features for that URL before you run it through the classifier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the decision tree based on the entropy criterion\n",
    "\n",
    "#Extract a row from the data\n",
    "\n",
    "#Make the prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Optional Step 5a:  Visualizing your Tree\n",
    "As an optional step, you can actually visualize your tree.  The following code will generate a graph of your decision tree.  You will need graphviz (http://www.graphviz.org) and pydotplus (or pydot) installed for this to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#These libraries are used to visualize the decision tree and require that you have GraphViz\n",
    "#and pydot or pydotplus installed on your computer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 6:  Assesing your Model's Accuracy\n",
    "Now that you have a model, the next step is to assess how accurate the model is.\n",
    "1.  Call the ```.predict()``` method with your training data and store the results in a new dataframe called ```training_predictions```.\n",
    "2.  Next, call the ```metrics.accuracy_score()``` method using your training target data and the predictions you just made.  \n",
    "3.  Print out the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#How accurate is the classifier?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Classification Report...  but this is all on known data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#K-fold validation\n",
    "Steps:\n",
    "1.  Partition the dataset into *k* different subsets\n",
    "2.  Create *k* different models by training on *k-1* subsets and testing on the remaining subsets\n",
    "3.  Measure the performance on each of the models and take the average measure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mean_score( scores ):\n",
    "    return \"Mean score: {0:.3f} (+/- {1:.3f})\".format( np.mean(scores), sem( scores ))\n",
    "\n",
    "print( mean_score( scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
